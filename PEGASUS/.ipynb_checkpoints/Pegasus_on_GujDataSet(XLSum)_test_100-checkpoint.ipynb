{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Darshan2104/Transformer-Adapter/blob/main/Pegasus_on_GujDataSet(XLSum)_test_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct inference with gujarati data\n",
    "- We have updated tokenizer and directly did inference on gujarati data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1hKQIGWt0dR",
    "outputId": "8b27e343-1982-48ad-e092-360a3cd12969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
      "Requirement already satisfied: inltk in /usr/local/lib/python3.7/dist-packages (0.9)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from inltk) (3.2.2)\n",
      "Requirement already satisfied: typing in /usr/local/lib/python3.7/dist-packages (from inltk) (3.7.4.3)\n",
      "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.0.0)\n",
      "Requirement already satisfied: bottleneck in /usr/local/lib/python3.7/dist-packages (from inltk) (1.3.2)\n",
      "Requirement already satisfied: fastai==1.0.57 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.0.57)\n",
      "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.7/dist-packages (from inltk) (2.2.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from inltk) (1.3.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from inltk) (4.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from inltk) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from inltk) (0.1.96)\n",
      "Requirement already satisfied: aiohttp>=3.5.4 in /usr/local/lib/python3.7/dist-packages (from inltk) (3.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from inltk) (21.3)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from inltk) (2.8.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from inltk) (7.1.2)\n",
      "Requirement already satisfied: async-timeout>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from inltk) (4.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from inltk) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from inltk) (6.0)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (from inltk) (7.352.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from inltk) (1.19.5)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (0.11.1+cu111)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==1.0.57->inltk) (1.10.0+cu111)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (2.0.11)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (1.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (3.10.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.5.4->inltk) (6.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (4.62.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (3.0.6)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (2.0.6)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (57.4.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.9.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0.18->inltk) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->inltk) (3.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->inltk) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->inltk) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->inltk) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->inltk) (2018.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install inltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W5k8fI__vWqo"
   },
   "outputs": [],
   "source": [
    "from transformers import PegasusModel,PegasusConfig, PegasusTokenizer, AutoTokenizer, AutoModel, PegasusForConditionalGeneration\n",
    "import torch\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Ki-CeJpvZZp"
   },
   "outputs": [],
   "source": [
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n",
    "# tokenizer = PegasusTokenizer(vocab_file='/content/drive/MyDrive/Tokenizer/gujarati_lm.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4eSYjJSO0_ds"
   },
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer(vocab_file='/content/drive/MyDrive/Tokenizer/gujarati_lm.model',name_or_path='PEGASUS', model_max_length=1024,bos_token='<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLhPP4iHSzxc",
    "outputId": "4e53aab5-95ab-423a-b037-7e60fc5c832a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '</s>': 105,\n",
       " '<mask_1>': 2,\n",
       " '<mask_2>': 3,\n",
       " '<unk_2>': 4,\n",
       " '<unk_3>': 5,\n",
       " '<unk_4>': 6,\n",
       " '<unk_5>': 7,\n",
       " '<unk_6>': 8,\n",
       " '<unk_7>': 9,\n",
       " '<unk_8>': 10,\n",
       " '<unk_9>': 11,\n",
       " '<unk_10>': 12,\n",
       " '<unk_11>': 13,\n",
       " '<unk_12>': 14,\n",
       " '<unk_13>': 15,\n",
       " '<unk_14>': 16,\n",
       " '<unk_15>': 17,\n",
       " '<unk_16>': 18,\n",
       " '<unk_17>': 19,\n",
       " '<unk_18>': 20,\n",
       " '<unk_19>': 21,\n",
       " '<unk_20>': 22,\n",
       " '<unk_21>': 23,\n",
       " '<unk_22>': 24,\n",
       " '<unk_23>': 25,\n",
       " '<unk_24>': 26,\n",
       " '<unk_25>': 27,\n",
       " '<unk_26>': 28,\n",
       " '<unk_27>': 29,\n",
       " '<unk_28>': 30,\n",
       " '<unk_29>': 31,\n",
       " '<unk_30>': 32,\n",
       " '<unk_31>': 33,\n",
       " '<unk_32>': 34,\n",
       " '<unk_33>': 35,\n",
       " '<unk_34>': 36,\n",
       " '<unk_35>': 37,\n",
       " '<unk_36>': 38,\n",
       " '<unk_37>': 39,\n",
       " '<unk_38>': 40,\n",
       " '<unk_39>': 41,\n",
       " '<unk_40>': 42,\n",
       " '<unk_41>': 43,\n",
       " '<unk_42>': 44,\n",
       " '<unk_43>': 45,\n",
       " '<unk_44>': 46,\n",
       " '<unk_45>': 47,\n",
       " '<unk_46>': 48,\n",
       " '<unk_47>': 49,\n",
       " '<unk_48>': 50,\n",
       " '<unk_49>': 51,\n",
       " '<unk_50>': 52,\n",
       " '<unk_51>': 53,\n",
       " '<unk_52>': 54,\n",
       " '<unk_53>': 55,\n",
       " '<unk_54>': 56,\n",
       " '<unk_55>': 57,\n",
       " '<unk_56>': 58,\n",
       " '<unk_57>': 59,\n",
       " '<unk_58>': 60,\n",
       " '<unk_59>': 61,\n",
       " '<unk_60>': 62,\n",
       " '<unk_61>': 63,\n",
       " '<unk_62>': 64,\n",
       " '<unk_63>': 65,\n",
       " '<unk_64>': 66,\n",
       " '<unk_65>': 67,\n",
       " '<unk_66>': 68,\n",
       " '<unk_67>': 69,\n",
       " '<unk_68>': 70,\n",
       " '<unk_69>': 71,\n",
       " '<unk_70>': 72,\n",
       " '<unk_71>': 73,\n",
       " '<unk_72>': 74,\n",
       " '<unk_73>': 75,\n",
       " '<unk_74>': 76,\n",
       " '<unk_75>': 77,\n",
       " '<unk_76>': 78,\n",
       " '<unk_77>': 79,\n",
       " '<unk_78>': 80,\n",
       " '<unk_79>': 81,\n",
       " '<unk_80>': 82,\n",
       " '<unk_81>': 83,\n",
       " '<unk_82>': 84,\n",
       " '<unk_83>': 85,\n",
       " '<unk_84>': 86,\n",
       " '<unk_85>': 87,\n",
       " '<unk_86>': 88,\n",
       " '<unk_87>': 89,\n",
       " '<unk_88>': 90,\n",
       " '<unk_89>': 91,\n",
       " '<unk_90>': 92,\n",
       " '<unk_91>': 93,\n",
       " '<unk_92>': 94,\n",
       " '<unk_93>': 95,\n",
       " '<unk_94>': 96,\n",
       " '<unk_95>': 97,\n",
       " '<unk_96>': 98,\n",
       " '<unk_97>': 99,\n",
       " '<unk_98>': 100,\n",
       " '<unk_99>': 101,\n",
       " '<unk_100>': 102,\n",
       " '<unk_101>': 103,\n",
       " '<unk_102>': 104,\n",
       " '.': 106,\n",
       " ',': 107,\n",
       " '▁છે': 108,\n",
       " '▁': 109,\n",
       " '▁અને': 110,\n",
       " 'ની': 111,\n",
       " 'માં': 112,\n",
       " 'ના': 113,\n",
       " '▁આ': 114,\n",
       " 'ને': 115,\n",
       " '▁જ': 116,\n",
       " '▁એક': 117,\n",
       " '▁તેમ': 118,\n",
       " '▁આવેલા': 119,\n",
       " 'નો': 120,\n",
       " 'નું': 121,\n",
       " '▁કે': 122,\n",
       " '▁ગામમાં': 123,\n",
       " '▁ખેતી': 124,\n",
       " '▁પણ': 125,\n",
       " '▁માટે': 126,\n",
       " 'થી': 127,\n",
       " '▁આવે': 128,\n",
       " '▁ભાગમાં': 129,\n",
       " '▁કરવામાં': 130,\n",
       " '▁તે': 131,\n",
       " 'ો': 132,\n",
       " 'ે': 133,\n",
       " '▁q': 134,\n",
       " '▁�': 135,\n",
       " 'ી': 136,\n",
       " '-': 137,\n",
       " '▁ગામ': 138,\n",
       " '▁પર': 139,\n",
       " '▁હતી': 140,\n",
       " '▁ભારત': 141,\n",
       " '▁ગુજરાત': 142,\n",
       " '▁મુખ્ય': 143,\n",
       " '▁જે': 144,\n",
       " 'X': 145,\n",
       " '▁આવેલું': 146,\n",
       " '▁હતા': 147,\n",
       " '▁સ્થાન': 148,\n",
       " 'એ': 149,\n",
       " '▁એ': 150,\n",
       " '▁હતો': 151,\n",
       " '▁પશ્ચિમ': 152,\n",
       " '▁ડેરી': 153,\n",
       " '▁રાજ્યના': 154,\n",
       " '▁દેશના': 155,\n",
       " '▁ભારતમાં': 156,\n",
       " '▁મુખ્યત્વે': 157,\n",
       " '▁સાથે': 158,\n",
       " '▁શાળા': 159,\n",
       " '▁હતું': 160,\n",
       " '▁જેવી': 161,\n",
       " 'ં': 162,\n",
       " '▁પ્રાથમિક': 163,\n",
       " '▁કરી': 164,\n",
       " '▁દૂધ': 165,\n",
       " '▁અન્ય': 166,\n",
       " '▁પશુપાલન': 167,\n",
       " '▁ગામના': 168,\n",
       " '▁હોય': 169,\n",
       " '▁ખેતમજૂરી': 170,\n",
       " '▁આંગણવાડી': 171,\n",
       " '▁વ્યવસાય': 172,\n",
       " '▁પંચાયતઘર': 173,\n",
       " '�': 174,\n",
       " '▁પ્રાપ્ય': 175,\n",
       " '▁સવલતો': 176,\n",
       " '▁તરીકે': 177,\n",
       " '▁લોકોનો': 178,\n",
       " '▁તાલુકામાં': 179,\n",
       " 'ગુજરાત': 180,\n",
       " '▁જિલ્લામાં': 181,\n",
       " '▁પાક': 182,\n",
       " '▁દ્વારા': 183,\n",
       " '▁કરે': 184,\n",
       " '▁થયેલી': 185,\n",
       " 'ા': 186,\n",
       " '▁શાકભાજીના': 187,\n",
       " '▁કુલ': 188,\n",
       " '▁એવા': 189,\n",
       " '▁થાય': 190,\n",
       " 'ક': 191,\n",
       " '▁રીતે': 192,\n",
       " 'ન': 193,\n",
       " 'સ': 194,\n",
       " '▁બાજરી': 195,\n",
       " 'ર': 196,\n",
       " '▁પૈકીના': 197,\n",
       " '▁લોકો': 198,\n",
       " '▁કપાસ': 199,\n",
       " '▁તેઓ': 200,\n",
       " '▁\"': 201,\n",
       " '▁તાલુકાઓ': 202,\n",
       " '▁તેના': 203,\n",
       " 'નાં': 204,\n",
       " '\"': 205,\n",
       " 'લ': 206,\n",
       " 'ઓ': 207,\n",
       " '▁ઘઉં': 208,\n",
       " '▁વધુ': 209,\n",
       " '▁આવી': 210,\n",
       " 's': 211,\n",
       " '▁અથવા': 212,\n",
       " '▁ના': 213,\n",
       " 'મ': 214,\n",
       " '▁ઉપયોગ': 215,\n",
       " '▁તેને': 216,\n",
       " '▁તેમણે': 217,\n",
       " '▁સુધી': 218,\n",
       " '▁નથી': 219,\n",
       " '▁સૌથી': 220,\n",
       " '▁બે': 221,\n",
       " 'તા': 222,\n",
       " '▁કરવા': 223,\n",
       " '▁તેમના': 224,\n",
       " '▁ન': 225,\n",
       " '▁કારણે': 226,\n",
       " ':': 227,\n",
       " '▁the': 228,\n",
       " '▁ઉત્તર': 229,\n",
       " '▁શકે': 230,\n",
       " '▁તેની': 231,\n",
       " '▁રજકો': 232,\n",
       " '્સ': 233,\n",
       " '▁જીરુ': 234,\n",
       " '▁પછી': 235,\n",
       " 'માંથી': 236,\n",
       " '▁માં': 237,\n",
       " '▁તો': 238,\n",
       " '▁જેમાં': 239,\n",
       " '▁તથા': 240,\n",
       " '▁જ્યારે': 241,\n",
       " '▁મધ્ય': 242,\n",
       " '▁અ': 243,\n",
       " '▁દક્ષિણ': 244,\n",
       " '▁કર્યો': 245,\n",
       " '▁દિવેલા': 246,\n",
       " '▁કરીને': 247,\n",
       " '▁પરંતુ': 248,\n",
       " '▁નામ': 249,\n",
       " '▁બાદ': 250,\n",
       " '▁પ્રથમ': 251,\n",
       " '▁ખાતે': 252,\n",
       " 'મી': 253,\n",
       " '▁વિસ્તારમાં': 254,\n",
       " '▁કર્યું': 255,\n",
       " '▁આવ્યું': 256,\n",
       " 'ત': 257,\n",
       " '▁સમાવેશ': 258,\n",
       " '▁કરતા': 259,\n",
       " '▁વર્ષ': 260,\n",
       " '▁થયો': 261,\n",
       " '▁સામાન્ય': 262,\n",
       " '▁‘': 263,\n",
       " '▁થઈ': 264,\n",
       " '▁મોટા': 265,\n",
       " '▁ત્યારે': 266,\n",
       " '▁કેટલાક': 267,\n",
       " '▁ઉપરાંત': 268,\n",
       " '▁શકાય': 269,\n",
       " 'જ': 270,\n",
       " '▁તેમજ': 271,\n",
       " '▁તેમની': 272,\n",
       " 'ડ': 273,\n",
       " '▁અહીં': 274,\n",
       " \"'\": 275,\n",
       " 'િત': 276,\n",
       " '▁આવ્યો': 277,\n",
       " '▁of': 278,\n",
       " '▁તેમને': 279,\n",
       " 'મા': 280,\n",
       " 'લા': 281,\n",
       " '▁ડાંગર': 282,\n",
       " '▁દરમિયાન': 283,\n",
       " '▁ધરાવે': 284,\n",
       " 'વા': 285,\n",
       " '▁આવ્યા': 286,\n",
       " '▁ભારતીય': 287,\n",
       " '▁ભાગ': 288,\n",
       " '▁સૌરાષ્ટ્ર': 289,\n",
       " 'લી': 290,\n",
       " '▁જિલ્લાના': 291,\n",
       " '▁રોજ': 292,\n",
       " '▁ખાસ': 293,\n",
       " 'પ': 294,\n",
       " 'આ': 295,\n",
       " 'ળ': 296,\n",
       " '▁પૂર્વ': 297,\n",
       " '▁ઓફ': 298,\n",
       " '▁જોવા': 299,\n",
       " 'વ': 300,\n",
       " '▁જેવા': 301,\n",
       " '▁જો': 302,\n",
       " '▁દિવસ': 303,\n",
       " '’': 304,\n",
       " '▁ને': 305,\n",
       " '▁જેમ': 306,\n",
       " '▁રહે': 307,\n",
       " '▁મુજબ': 308,\n",
       " 'જી': 309,\n",
       " '▁વગેરે': 310,\n",
       " '▁મંદિર': 311,\n",
       " '▁તલ': 312,\n",
       " 'રા': 313,\n",
       " '▁પોતાના': 314,\n",
       " '▁મગફળી': 315,\n",
       " '▁મળે': 316,\n",
       " 'ટ': 317,\n",
       " '▁ચણા': 318,\n",
       " 'ઝ': 319,\n",
       " 'રી': 320,\n",
       " 'ડી': 321,\n",
       " '▁થઇ': 322,\n",
       " '▁રાજ્ય': 323,\n",
       " ';': 324,\n",
       " '▁ધ': 325,\n",
       " '▁and': 326,\n",
       " '▁વચ્ચે': 327,\n",
       " '▁એમ': 328,\n",
       " '▁મકાઈ': 329,\n",
       " '▁સમય': 330,\n",
       " 'ણ': 331,\n",
       " '▁થી': 332,\n",
       " '▁તેમાં': 333,\n",
       " '▁જાય': 334,\n",
       " '▁કામ': 335,\n",
       " 'ય': 336,\n",
       " '▁વિવિધ': 337,\n",
       " '▁તાલુકા': 338,\n",
       " '▁સ': 339,\n",
       " 'ડા': 340,\n",
       " 'બ': 341,\n",
       " '▁અલગ': 342,\n",
       " '▁કોઈ': 343,\n",
       " '▁વસ્તી': 344,\n",
       " 'િક': 345,\n",
       " '▁ઇ': 346,\n",
       " '▁આવેલ': 347,\n",
       " '▁દિવેલી': 348,\n",
       " \"▁'\": 349,\n",
       " '▁બી': 350,\n",
       " '▁•': 351,\n",
       " '▁ઘણા': 352,\n",
       " '▁in': 353,\n",
       " '▁સામે': 354,\n",
       " '▁થયા': 355,\n",
       " '▁ધરાવતા': 356,\n",
       " 'ીય': 357,\n",
       " '▁ત્રણ': 358,\n",
       " '▁થયું': 359,\n",
       " '▁શહેર': 360,\n",
       " 'વી': 361,\n",
       " '▁બીજા': 362,\n",
       " 'ed': 363,\n",
       " '▁તુવર': 364,\n",
       " 'ું': 365,\n",
       " '▁તેનો': 366,\n",
       " '▁કરવાની': 367,\n",
       " '▁ત્યાં': 368,\n",
       " 'શ': 369,\n",
       " '▁ઉપર': 370,\n",
       " '▁નો': 371,\n",
       " '▁બની': 372,\n",
       " 'ુ': 373,\n",
       " 'દ': 374,\n",
       " 'ઇ': 375,\n",
       " '▁પોતાની': 376,\n",
       " '▁ની': 377,\n",
       " '▁શરૂ': 378,\n",
       " '▁વધારે': 379,\n",
       " '▁આદિવાસી': 380,\n",
       " '▁ઘણી': 381,\n",
       " '▁કરતાં': 382,\n",
       " 'ગ': 383,\n",
       " '▁૬': 384,\n",
       " '▁ફિલ્મ': 385,\n",
       " '▁તરફ': 386,\n",
       " '▁નદી': 387,\n",
       " '▁ગુજરાતી': 388,\n",
       " '▁માત્ર': 389,\n",
       " '▁ઈ': 390,\n",
       " '▁a': 391,\n",
       " 'સી': 392,\n",
       " 'તી': 393,\n",
       " '▁છતાં': 394,\n",
       " '▁આપવામાં': 395,\n",
       " '્': 396,\n",
       " '▁ભાષા': 397,\n",
       " '▁કારણ': 398,\n",
       " 'િયા': 399,\n",
       " '/': 400,\n",
       " '▁તેણે': 401,\n",
       " 'કો': 402,\n",
       " '▁હતાં': 403,\n",
       " '▁ગયા': 404,\n",
       " '▁શાકભાજી': 405,\n",
       " 'િ': 406,\n",
       " '▁વખત': 407,\n",
       " '▁જન્મ': 408,\n",
       " '▁દર': 409,\n",
       " '▁ટકા': 410,\n",
       " '▁કર્યા': 411,\n",
       " '▁દરેક': 412,\n",
       " '▁તમામ': 413,\n",
       " '▁to': 414,\n",
       " '▁સમયે': 415,\n",
       " '▁આવેલી': 416,\n",
       " '▁કોઇ': 417,\n",
       " '▁લગભગ': 418,\n",
       " '▁મે': 419,\n",
       " '▁મળી': 420,\n",
       " '▁ભારતના': 421,\n",
       " 'કા': 422,\n",
       " '▁તેનું': 423,\n",
       " 'ચ': 424,\n",
       " '▁હેઠળ': 425,\n",
       " '▁દૂર': 426,\n",
       " '▁રહી': 427,\n",
       " '▁જોકે': 428,\n",
       " '▁મેળવી': 429,\n",
       " '▁વિસ્તાર': 430,\n",
       " 'ંગ': 431,\n",
       " '▁જેને': 432,\n",
       " 'ઈ': 433,\n",
       " '્યા': 434,\n",
       " '▁૧૦': 435,\n",
       " 'હ': 436,\n",
       " 'પુર': 437,\n",
       " '▁દેશ': 438,\n",
       " '▁“': 439,\n",
       " 'ોમાં': 440,\n",
       " '▁જ્યાં': 441,\n",
       " '▁હોવા': 442,\n",
       " '▁પ્રમાણે': 443,\n",
       " '▁વરિયાળી': 444,\n",
       " '▁ખૂબ': 445,\n",
       " '%': 446,\n",
       " '▁વિ': 447,\n",
       " '▁પાસે': 448,\n",
       " '▁આપી': 449,\n",
       " 'િંગ': 450,\n",
       " 'રો': 451,\n",
       " 'ટી': 452,\n",
       " '▁શબ્દ': 453,\n",
       " '▁રહ્યા': 454,\n",
       " '▁મથક': 455,\n",
       " '▁નજીક': 456,\n",
       " '▁મૂળ': 457,\n",
       " '▁મોટી': 458,\n",
       " '▁ચાર': 459,\n",
       " '▁યુદ્ધ': 460,\n",
       " '▁પહેલા': 461,\n",
       " '▁એટલે': 462,\n",
       " '▁જાહેર': 463,\n",
       " '▁રચના': 464,\n",
       " '▁આપે': 465,\n",
       " '▁બહાર': 466,\n",
       " '▁નવા': 467,\n",
       " '▁અભ્યાસ': 468,\n",
       " '▁બંને': 469,\n",
       " '▁અનેક': 470,\n",
       " '▁-': 471,\n",
       " '▁પાંચ': 472,\n",
       " 'ોને': 473,\n",
       " '▁યુ': 474,\n",
       " 'યા': 475,\n",
       " '▁પરથી': 476,\n",
       " '▁આમ': 477,\n",
       " '▁કંપની': 478,\n",
       " '▁સેવા': 479,\n",
       " '▁રાષ્ટ્રીય': 480,\n",
       " 'સા': 481,\n",
       " '▁અંગે': 482,\n",
       " '▁વિકાસ': 483,\n",
       " 'ફ': 484,\n",
       " '▁૧૧': 485,\n",
       " '▁શેરડી': 486,\n",
       " '▁તમાકુ': 487,\n",
       " 'મો': 488,\n",
       " 'થ': 489,\n",
       " '્યો': 490,\n",
       " '▁જેવાં': 491,\n",
       " '▁એવી': 492,\n",
       " '▁અસર': 493,\n",
       " '▁વિશ્વ': 494,\n",
       " '▁લે': 495,\n",
       " 'એસ': 496,\n",
       " 'અ': 497,\n",
       " '▁૯': 498,\n",
       " '▁કહેવાય': 499,\n",
       " '▁પાણી': 500,\n",
       " 'તો': 501,\n",
       " '▁નીચે': 502,\n",
       " '▁સરકાર': 503,\n",
       " 'કે': 504,\n",
       " '▁જીવન': 505,\n",
       " '▁ધર્મ': 506,\n",
       " '2': 507,\n",
       " '▁બંધ': 508,\n",
       " '▁માર્ગ': 509,\n",
       " '▁મૃત્યુ': 510,\n",
       " '▁સુ': 511,\n",
       " '▁જિલ્લો': 512,\n",
       " '▁નહીં': 513,\n",
       " '▁અનુસાર': 514,\n",
       " '▁પ્રદેશ': 515,\n",
       " 'ing': 516,\n",
       " '▁ઉત્પાદન': 517,\n",
       " '▁પડે': 518,\n",
       " 'રે': 519,\n",
       " '▁સ્થાપના': 520,\n",
       " '▁સ્થળ': 521,\n",
       " '▁રાજા': 522,\n",
       " '▁પ્ર': 523,\n",
       " 'ીયા': 524,\n",
       " '▁મ': 525,\n",
       " '▁દિશામાં': 526,\n",
       " '▁કાર્ય': 527,\n",
       " '▁બનાવવામાં': 528,\n",
       " '▁એવું': 529,\n",
       " '▁ઉપલબ્ધ': 530,\n",
       " '▁હોવાથી': 531,\n",
       " '▁પાન': 532,\n",
       " '▁નાના': 533,\n",
       " '”': 534,\n",
       " '▁પ': 535,\n",
       " '▁કા': 536,\n",
       " '▁મા': 537,\n",
       " 'ખ': 538,\n",
       " '▁દિવસે': 539,\n",
       " 'બી': 540,\n",
       " '▁લેવામાં': 541,\n",
       " '▁કહે': 542,\n",
       " '▁સમગ્ર': 543,\n",
       " '▁વસે': 544,\n",
       " '▁વ': 545,\n",
       " '▁કેટલીક': 546,\n",
       " '▁આધારિત': 547,\n",
       " '▁માહિતી': 548,\n",
       " '▁શ્રી': 549,\n",
       " 'ાઈ': 550,\n",
       " '▁કેન્દ્ર': 551,\n",
       " '▁શરૂઆત': 552,\n",
       " '▁અમુક': 553,\n",
       " '▁આગળ': 554,\n",
       " 'પી': 555,\n",
       " '▁પ્રકારના': 556,\n",
       " '▁વ્યક્તિ': 557,\n",
       " 'પર': 558,\n",
       " '▁૧૪': 559,\n",
       " '▁અર્થ': 560,\n",
       " '▁સુરત': 561,\n",
       " '▁કેળાં': 562,\n",
       " 'ધ': 563,\n",
       " '▁શિક્ષણ': 564,\n",
       " '▁૮': 565,\n",
       " '▁મદદ': 566,\n",
       " '▁લેખ': 567,\n",
       " '▁18': 568,\n",
       " '▁છ': 569,\n",
       " '▁યુનાઇટેડ': 570,\n",
       " '▁સમયમાં': 571,\n",
       " '▁પ્રાચીન': 572,\n",
       " '▁રાજ': 573,\n",
       " 'y': 574,\n",
       " 'વે': 575,\n",
       " '▁પ્રાપ્ત': 576,\n",
       " '▁કો': 577,\n",
       " 'પુરા': 578,\n",
       " '▁૧': 579,\n",
       " '▁મગ': 580,\n",
       " '▁તેવી': 581,\n",
       " '▁ઓ': 582,\n",
       " '▁આવતા': 583,\n",
       " 'લે': 584,\n",
       " '▁પ્રમાણમાં': 585,\n",
       " '▁સગવડ': 586,\n",
       " '▁નગર': 587,\n",
       " '▁તેવા': 588,\n",
       " '▁કિ': 589,\n",
       " '▁જુવાર': 590,\n",
       " '▁is': 591,\n",
       " 'd': 592,\n",
       " '▁પહેલાં': 593,\n",
       " 'ીંગ': 594,\n",
       " '્ય': 595,\n",
       " '▁જમીન': 596,\n",
       " '▁ભગવાન': 597,\n",
       " '▁૫': 598,\n",
       " '▁સમાન': 599,\n",
       " 'દા': 600,\n",
       " '▁ક': 601,\n",
       " '▁હવે': 602,\n",
       " '▁સ્થાનિક': 603,\n",
       " '▁ક્ષેત્ર': 604,\n",
       " '▁મહત્વ': 605,\n",
       " 'બા': 606,\n",
       " '▁બીજી': 607,\n",
       " '▁જિલ્લાનું': 608,\n",
       " '▁દેવ': 609,\n",
       " 'લો': 610,\n",
       " '▁નિર્માણ': 611,\n",
       " 'દુધની': 612,\n",
       " '▁પૈકી': 613,\n",
       " 'નુ': 614,\n",
       " '▁કચ્છ': 615,\n",
       " '▁ત્યાર': 616,\n",
       " '▁ખેત': 617,\n",
       " '▁સે': 618,\n",
       " '▁બ': 619,\n",
       " '▁આશરે': 620,\n",
       " '▁નોકરી': 621,\n",
       " '▁લોકોની': 622,\n",
       " '▁અત્યંત': 623,\n",
       " '▁વખતે': 624,\n",
       " '▁1': 625,\n",
       " '▁૭': 626,\n",
       " '▁આધુનિક': 627,\n",
       " 'વો': 628,\n",
       " '▁એન્ડ': 629,\n",
       " '▁૪': 630,\n",
       " '▁રજૂ': 631,\n",
       " '▁વધારો': 632,\n",
       " '▁કાર્યો': 633,\n",
       " '▁ઓછા': 634,\n",
       " '▁પોતાનું': 635,\n",
       " '▁પ્રદેશમાં': 636,\n",
       " '▁શક્કરીયાં': 637,\n",
       " '▁ચોક્કસ': 638,\n",
       " '▁પ્રક્રિયા': 639,\n",
       " '્યુ': 640,\n",
       " '▁શોધ': 641,\n",
       " '▁હિંદુ': 642,\n",
       " '▁જંગલ': 643,\n",
       " '▁વડે': 644,\n",
       " '▁જેના': 645,\n",
       " '▁જરૂર': 646,\n",
       " '▁થતો': 647,\n",
       " 'ોની': 648,\n",
       " '▁બનાવવા': 649,\n",
       " 'સ્': 650,\n",
       " '▁તેથી': 651,\n",
       " 'n': 652,\n",
       " 'તે': 653,\n",
       " '▁મહારાષ્ટ્ર': 654,\n",
       " '▁રાજ્યમાં': 655,\n",
       " '▁રંગ': 656,\n",
       " '▁તૈયાર': 657,\n",
       " '▁યોગ્ય': 658,\n",
       " '▁૧૮': 659,\n",
       " 'વામાં': 660,\n",
       " '▁અમદાવાદ': 661,\n",
       " '▁નવી': 662,\n",
       " '▁માનવ': 663,\n",
       " 'e': 664,\n",
       " '▁બટાટા': 665,\n",
       " 'er': 666,\n",
       " 'ાઇ': 667,\n",
       " '▁પદ્ધતિ': 668,\n",
       " 'ન્ટ': 669,\n",
       " '▁હાથ': 670,\n",
       " 'જે': 671,\n",
       " '▁અંત': 672,\n",
       " '▁સી': 673,\n",
       " '▁સંગીત': 674,\n",
       " '▁ડુંગરા': 675,\n",
       " '▁ઉ': 676,\n",
       " '▁ચાલુ': 677,\n",
       " 'સર': 678,\n",
       " '▁પ્રકારની': 679,\n",
       " '▁આપ્યો': 680,\n",
       " '▁સાહિત્ય': 681,\n",
       " '▁રહ્યો': 682,\n",
       " '▁અંગ્રેજી': 683,\n",
       " '▁ચા': 684,\n",
       " '▁ગયો': 685,\n",
       " 'ર્સ': 686,\n",
       " 'સે': 687,\n",
       " '▁ઓછી': 688,\n",
       " '▁કર': 689,\n",
       " 'વર': 690,\n",
       " '▁વિશાળ': 691,\n",
       " 'કી': 692,\n",
       " '▁બ્રિટિશ': 693,\n",
       " 'વિ': 694,\n",
       " 'ોના': 695,\n",
       " '▁ઓળખાય': 696,\n",
       " '્યું': 697,\n",
       " '▁દરમ્યાન': 698,\n",
       " '▁બનાસકાંઠા': 699,\n",
       " '▁માનવામાં': 700,\n",
       " '▁પુત્ર': 701,\n",
       " '▁બન્યા': 702,\n",
       " '▁સુધારો': 703,\n",
       " 't': 704,\n",
       " '▁આંતરરાષ્ટ્રીય': 705,\n",
       " '▁વડોદરા': 706,\n",
       " '▁જેથી': 707,\n",
       " '▁ભૂમિકા': 708,\n",
       " '▁સંખ્યા': 709,\n",
       " '▁લોકપ્રિય': 710,\n",
       " '▁આદિવાસીઓ': 711,\n",
       " '▁વાત': 712,\n",
       " 'C': 713,\n",
       " '▁હોવાનું': 714,\n",
       " '▁પી': 715,\n",
       " 'વાળા': 716,\n",
       " '▁આવેલો': 717,\n",
       " '▁વસવાટ': 718,\n",
       " 'ડો': 719,\n",
       " 'જા': 720,\n",
       " 'કાર': 721,\n",
       " 'ોએ': 722,\n",
       " '▁The': 723,\n",
       " '▁પૂરી': 724,\n",
       " '▁ફરી': 725,\n",
       " '▁સૈનિકો': 726,\n",
       " '▁ઘણાં': 727,\n",
       " '▁ડી': 728,\n",
       " '▁આપ્યું': 729,\n",
       " '▁વ્યવસાયમાં': 730,\n",
       " 'ણી': 731,\n",
       " '▁પિતા': 732,\n",
       " '▁ભાવનગર': 733,\n",
       " '▁બધા': 734,\n",
       " '▁કહેવા': 735,\n",
       " '’,': 736,\n",
       " '▁રહ્યું': 737,\n",
       " 'al': 738,\n",
       " '▁ઉલ્લેખ': 739,\n",
       " '▁બને': 740,\n",
       " 'ખા': 741,\n",
       " '▁થતા': 742,\n",
       " 'કર': 743,\n",
       " '▁સિ': 744,\n",
       " '▁2': 745,\n",
       " '▁ઉદાહરણ': 746,\n",
       " 'ન્સ': 747,\n",
       " '▁બોલી': 748,\n",
       " 'વું': 749,\n",
       " '▁શાસન': 750,\n",
       " '▁બિન': 751,\n",
       " '▁કી': 752,\n",
       " '▁એવો': 753,\n",
       " '▁રે': 754,\n",
       " '▁2007': 755,\n",
       " '▁રોગ': 756,\n",
       " '▁થયેલ': 757,\n",
       " '▁અહીંના': 758,\n",
       " '▁સિવાય': 759,\n",
       " '▁10': 760,\n",
       " '▁હાલમાં': 761,\n",
       " '▁નિ': 762,\n",
       " '▁કરતી': 763,\n",
       " 'યુ': 764,\n",
       " '▁ત્યારબાદ': 765,\n",
       " 'ટા': 766,\n",
       " '▁વ્યવસ્થા': 767,\n",
       " '▁રહેલા': 768,\n",
       " '▁તેમનો': 769,\n",
       " '▁2008': 770,\n",
       " '▁સુધીમાં': 771,\n",
       " '1': 772,\n",
       " '▁વાર': 773,\n",
       " '▁થયેલા': 774,\n",
       " '▁૨': 775,\n",
       " 'ક્સ': 776,\n",
       " '▁પાડે': 777,\n",
       " '▁વર્ષના': 778,\n",
       " '▁બાળકો': 779,\n",
       " '▁લગ્ન': 780,\n",
       " '▁નક્કી': 781,\n",
       " '▁20': 782,\n",
       " '▁થતી': 783,\n",
       " '▁પૂર્ણ': 784,\n",
       " '▁લાંબા': 785,\n",
       " 'ઓને': 786,\n",
       " '▁દિલ્હી': 787,\n",
       " '▁બહુ': 788,\n",
       " '▁ધ્યાન': 789,\n",
       " '▁માતા': 790,\n",
       " '▁ધરાવતી': 791,\n",
       " '▁ઉચ્ચ': 792,\n",
       " '▁આજે': 793,\n",
       " '▁શહેરમાં': 794,\n",
       " '▁હ': 795,\n",
       " 'es': 796,\n",
       " 'ેલા': 797,\n",
       " 'ly': 798,\n",
       " '▁ભાગના': 799,\n",
       " '▁સત્તા': 800,\n",
       " '▁ઓળખવા': 801,\n",
       " '▁જાણીતા': 802,\n",
       " '▁રામ': 803,\n",
       " 'ીક': 804,\n",
       " '▁as': 805,\n",
       " '▁મોટો': 806,\n",
       " '▁ઘર': 807,\n",
       " '▁૩': 808,\n",
       " '▁કહ્યું': 809,\n",
       " '°': 810,\n",
       " '▁કલા': 811,\n",
       " '▁3': 812,\n",
       " '▁આર્થિક': 813,\n",
       " '▁લેવા': 814,\n",
       " '▁પડી': 815,\n",
       " '▁સ્વરૂપ': 816,\n",
       " 'નિ': 817,\n",
       " 'ઉ': 818,\n",
       " '▁આવતી': 819,\n",
       " '▁મુંબઈ': 820,\n",
       " '▁પ્રમાણ': 821,\n",
       " '▁દા': 822,\n",
       " '▁પસાર': 823,\n",
       " '▁જણાવ્યું': 824,\n",
       " '▁અમેરિકન': 825,\n",
       " '▁અંદર': 826,\n",
       " 'મે': 827,\n",
       " '▁સ્થિત': 828,\n",
       " '▁ક્ષમતા': 829,\n",
       " '▁સાત': 830,\n",
       " '▁દે': 831,\n",
       " '▁સ્તર': 832,\n",
       " '▁પિયત': 833,\n",
       " '▁સતત': 834,\n",
       " '▁ગો': 835,\n",
       " '▁કરવાનો': 836,\n",
       " 'o': 837,\n",
       " '▁જેટલા': 838,\n",
       " '▁સંપૂર્ણ': 839,\n",
       " '▁વર્ષે': 840,\n",
       " 'તિ': 841,\n",
       " '▁જેટલી': 842,\n",
       " '▁યોજના': 843,\n",
       " '▁જરૂરી': 844,\n",
       " '▁લીધે': 845,\n",
       " 'શે': 846,\n",
       " '▁મહા': 847,\n",
       " '▁લા': 848,\n",
       " 'સ્ટ': 849,\n",
       " 'ૂ': 850,\n",
       " '▁હે': 851,\n",
       " '▁ઉપ': 852,\n",
       " 'r': 853,\n",
       " 'યો': 854,\n",
       " 'ઠી': 855,\n",
       " '▁રાજકીય': 856,\n",
       " 'ણા': 857,\n",
       " '▁નેટવર્ક': 858,\n",
       " '▁સંશોધન': 859,\n",
       " '▁પૃથ્વી': 860,\n",
       " '▁ભાગે': 861,\n",
       " '▁વર્ષની': 862,\n",
       " '▁ધાર્મિક': 863,\n",
       " '▁ક્રિકેટ': 864,\n",
       " '4': 865,\n",
       " '▁સં': 866,\n",
       " '▁ત': 867,\n",
       " 'ભ': 868,\n",
       " '▁વિચાર': 869,\n",
       " '▁લઈ': 870,\n",
       " '▁ગ': 871,\n",
       " '▁આપવા': 872,\n",
       " '▁પ્રખ્યાત': 873,\n",
       " '▁નોંધપાત્ર': 874,\n",
       " 'ીઓ': 875,\n",
       " '▁15': 876,\n",
       " '▁દેશોમાં': 877,\n",
       " '▁જેઓ': 878,\n",
       " '▁સામાજિક': 879,\n",
       " 'લિ': 880,\n",
       " '▁16': 881,\n",
       " '▁નામે': 882,\n",
       " '▁વિશે': 883,\n",
       " '▁17': 884,\n",
       " '▁મોટું': 885,\n",
       " '▁નામના': 886,\n",
       " '▁આસપાસ': 887,\n",
       " '▁ગયું': 888,\n",
       " '▁પ્રસિદ્ધ': 889,\n",
       " 'ઘ': 890,\n",
       " '▁સ્ટેટ્સ': 891,\n",
       " 'ઓમાં': 892,\n",
       " '▁સપાટી': 893,\n",
       " '▁જાન્યુઆરી': 894,\n",
       " '▁સૂર્ય': 895,\n",
       " 'p': 896,\n",
       " '▁સર': 897,\n",
       " '▁મુ': 898,\n",
       " '▁જાહેરાત': 899,\n",
       " '▁શ્રેષ્ઠ': 900,\n",
       " '▁નાની': 901,\n",
       " '▁સદી': 902,\n",
       " '▁બનાવી': 903,\n",
       " '▁રાજકોટ': 904,\n",
       " '▁આપ': 905,\n",
       " 'ેલી': 906,\n",
       " 'દાર': 907,\n",
       " 'વાદ': 908,\n",
       " '▁થોડા': 909,\n",
       " '▁રાખવામાં': 910,\n",
       " '▁અરવલ્લી': 911,\n",
       " 'ડે': 912,\n",
       " '▁સો': 913,\n",
       " 'ીત': 914,\n",
       " '▁વે': 915,\n",
       " '▁કામગીરી': 916,\n",
       " '▁રસ': 917,\n",
       " '▁યુનિવર્સિટી': 918,\n",
       " '▁પુરસ્કાર': 919,\n",
       " '▁છોટાઉદેપુર': 920,\n",
       " '▁પ્રમુખ': 921,\n",
       " '▁જેનો': 922,\n",
       " '▁રિ': 923,\n",
       " 'િસ': 924,\n",
       " '▁આધાર': 925,\n",
       " '▁ફક્ત': 926,\n",
       " '▁ફુલ': 927,\n",
       " '▁સફળ': 928,\n",
       " '▁પ્રવેશ': 929,\n",
       " '▁સ્ટેશન': 930,\n",
       " '▁s': 931,\n",
       " '▁લોકોને': 932,\n",
       " '▁થવા': 933,\n",
       " '▁ઇતિહાસ': 934,\n",
       " '▁2009': 935,\n",
       " '▁લીધો': 936,\n",
       " '▁4': 937,\n",
       " '▁મિ': 938,\n",
       " '.\"': 939,\n",
       " 'વાડા': 940,\n",
       " '▁જોઇએ': 941,\n",
       " '▁વી': 942,\n",
       " '▁બા': 943,\n",
       " '▁પુસ્તક': 944,\n",
       " 'મ્': 945,\n",
       " '▁જ્ઞાન': 946,\n",
       " '▁સમાજ': 947,\n",
       " '▁વર્ષો': 948,\n",
       " 'મિ': 949,\n",
       " '▁મીટર': 950,\n",
       " 'પા': 951,\n",
       " '▁ડિ': 952,\n",
       " 'દી': 953,\n",
       " '▁કાર': 954,\n",
       " '▁વિભાગ': 955,\n",
       " '▁પ્રેમ': 956,\n",
       " '▁પ્રતિ': 957,\n",
       " '▁S': 958,\n",
       " '▁લાલ': 959,\n",
       " 'ગો': 960,\n",
       " '▁આધારે': 961,\n",
       " 'in': 962,\n",
       " ')': 963,\n",
       " 'ર્': 964,\n",
       " '▁કરવાનું': 965,\n",
       " '▁રમત': 966,\n",
       " '▁કમ્પ્યુટર': 967,\n",
       " '▁જતા': 968,\n",
       " 'a': 969,\n",
       " '▁નવ': 970,\n",
       " '▁લીધી': 971,\n",
       " 'b': 972,\n",
       " '▁આવા': 973,\n",
       " '▁અગાઉ': 974,\n",
       " '▁રેલ્વે': 975,\n",
       " '▁ભારે': 976,\n",
       " '▁સ્થિતિ': 977,\n",
       " '▁સંયુક્ત': 978,\n",
       " '▁સદીના': 979,\n",
       " '▁તળાવ': 980,\n",
       " '▁સંસ્થા': 981,\n",
       " '▁સભ્ય': 982,\n",
       " '▁ડે': 983,\n",
       " '▁એપ્રિલ': 984,\n",
       " '▁પાછળ': 985,\n",
       " '▁સંસ્કૃતિ': 986,\n",
       " '▁ટીમ': 987,\n",
       " '્ડ': 988,\n",
       " '▁કાયદા': 989,\n",
       " '▁દબાણ': 990,\n",
       " '▁સંગ્રહ': 991,\n",
       " 'ેશ્વર': 992,\n",
       " 'બે': 993,\n",
       " 'ાં': 994,\n",
       " '▁કુ': 995,\n",
       " '▁સ્': 996,\n",
       " 'ેશન': 997,\n",
       " '▁૧૨': 998,\n",
       " '▁બદલે': 999,\n",
       " '▁મહ': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B2SsZ6vt2ddb"
   },
   "outputs": [],
   "source": [
    "# tokenizer.model_max_length = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FGJ2Hay4sQA",
    "outputId": "8a1f4662-e63f-4971-eae2-decf25792996"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='PEGASUS', vocab_size=20103, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGEVg9Zr_Xex",
    "outputId": "bd781c8a-0a88-4a5b-9490-493f611bfce7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79E_z4aN_WQq",
    "outputId": "186ec160-f683-4ca9-f50c-94bfdaa082bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(20103, 1024)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MEOotJh_mFE",
    "outputId": "7eeaac43-0e22-46c5-d009-d6eca4da101e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PegasusForConditionalGeneration(\n",
       "  (model): PegasusModel(\n",
       "    (shared): Embedding(20103, 1024)\n",
       "    (encoder): PegasusEncoder(\n",
       "      (embed_tokens): Embedding(20103, 1024)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusEncoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): PegasusDecoder(\n",
       "      (embed_tokens): Embedding(20103, 1024)\n",
       "      (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): PegasusDecoderLayer(\n",
       "          (self_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): PegasusAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=20103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKcmgRR8rd_6",
    "outputId": "29376fac-4946-47d3-fc9c-3bb925989a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.11)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "b2b9ae59eeff477b967990aea69d5638",
      "02c0113144ca4a3c801ee16450bd23fb",
      "dab4563495f644c48c3a8641df375a86",
      "d7c802ccaf214080aca4b6b8742ded3b",
      "8b08b8a13f1648c99ff74245413a81fd",
      "16f9e10417b849d88df2d9a8f951f5dd",
      "669539e1d571432cbcd46302c56fd3a1",
      "3cfc34fabd4d4950a74083cfbd3ae5e8",
      "9fa56ee3d68c4c769e616d43f81357ff",
      "bf84befce4064e6d8bad584a58f2148e",
      "6e31b9935da74636985f5b7d550291d8"
     ]
    },
    "id": "2tNoPXbrrajv",
    "outputId": "f791e89c-fee5-48bd-b064-5458569760f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xlsum (/root/.cache/huggingface/datasets/GEM___xlsum/gujarati/2.0.0/c5f94b79254b76efed292f24957fe663c4b35c83e91284fbefc51adf4aea8dc0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b9ae59eeff477b967990aea69d5638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"GEM/xlsum\",'gujarati')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4Fo98Uzrvr7",
    "outputId": "2931dcaa-91ce-47f6-f96b-c799a32675c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['પાકિસ્તાનમાં ફસાયેલા ભારતીય નાગરિકો ખેડૂત પરિવાર સાથે સંબંધ ધરાવતા ઇશાકભાઈ બોકડા પાકિસ્તાનના કરાચીમાં એક લગ્નપ્રસંગમાં સામેલ થવા માટે 11 માર્ચે ભારતથી ગયા હતા. તેમનું કહેવું છે કે માર્ચમાં જ તેમને ત્યાંથી પરત આવવાનું હતું, પરંતુ લૉકડાઉનને કારણે 26 લોકો ત્યાં ફસાઈ ગયા છે. ઇશાકભાઈ અને તેમની સાથેના અન્ય ભારતીય નાગરિકો પણ ભારત પરત આવવા માગે છે અને એ માટે તેમણે ભારત સરકારની મદદ પણ માગી છે. તેમણે બીબીસી સાથે વાતચીતમાં કહ્યું કે અમે ઇસ્લામાબાદમાં ભારતીય હાઈકમિશનને અરજી કરી છે, પરંતુ હજી સુધી કોઈ ઠોસ માહિતી નથી મળી. બીબીસીએ આ અંગે વાત કરવા માટે ભારતીય વિદેશમંત્રાલયનો સંપર્ક કર્યો હતો, પરંતુ આ લખાઈ રહ્યું છે ત્યાં સુધી કોઈ જવાબ મળી શક્યો નથી. ઇશાકભાઈ કહે છે કે અમૃતસરથી ગુજરાત આવવા માટે તેમણે ચાર જૂનની ટ્રેનની ટિકિટનું બુકિંગ પણ કરાવી લીધું છે અને અન્ય તમામ વ્યવસ્થા થઈ ગઈ છે, પરંતુ પાકિસ્તાનથી ભારત આવવા માટે અટારી-વાઘા સરહદ પાર કરવાની પરવાનગી મળી શકી નથી. ગોધરાના આ પરિવારે રમઝાન અને ઈદ પણ પાકિસ્તાનમાં ઊજવી છે. ઇશાકભાઈ જણાવે છે કે \"રમઝાન અને ઈદ પણ અમે પરિવારથી દૂર અહીં પાકિસ્તાનમાં ઊજવી છે, પણ હવે ઘરે જવું છે.\" ઇશાકભાઈ પોતાનાં પત્ની, પુત્રી, ભાણેજ અને અન્ય બે લોકો સાથે પાકિસ્તાનના કરાચીમાં છે. તેમણે કહ્યું, \"ગોધરાથી વરરાજા સાથે 26 લોકો બારાતમાં પાકિસ્તાનના કરાચી ગયા હતા. નિકાહ માર્ચની 14 તારીખે પઢવામાં આવ્યા હતા. દુલ્હા અને દુલ્હનને જરૂરી દસ્તાવેજ તૈયાર કરીને થોડો સમય પાકિસ્તાનમાં રોકાવાનું હતું, પરંતુ બાકીના લોકો જે નિકાહમાં સામેલ થવા માટે ભારતથી ગયા હતા તેમને પાછું આવવાનું હતું.\" તેઓ જણાવે છે કે 22 માર્ચે જનતા કર્ફ્યુ જાહેર થયા પછી બૉર્ડર બંધ કરવામાં આવી હતી એટલે તેઓ પાછા ફરી ન શક્યા. ઇશાકભાઈ વધુમાં કહે છે કે પાકિસ્તાનમાં નવવિવાહિત દંપતી તો સાથે છે, પરંતુ તેમની બારાતમાં આવેલા નવયુવાનોના પરિવારો ભારતમાં તેમની રાહ જોઈ રહ્યા છે. \\'અબ્બુ ક્યારે ઘરે આવશો?\\' પાકિસ્તાનમાં ફસાયેલા ભારતીય નાગરિકો તો ગોધરામાં દરજીકામ કરતા ઇમરાનભાઈનું કહે છે, \"વતન તો વતન છે, બાળકો ઘરે રાહ જોઈ રહ્યાં છે.\" ઇમરાનભાઈ તેમનાં પત્ની આયેશા અને સાસુ મેહરુનિસ્સા સાથે કરાચીમાં એક નિકાહમાં સામેલ થવા માટે ફેબ્રુઆરીની 28 તારીખે ગયા હતા. તેમણે બીબીસીને જણાવ્યું કે તેઓ કરાચીમાં પોતાનાં ફોઈનાં પુત્રીનાં નિકાહમાં ગયા હતા. 19 માર્ચે પાછું આવવાનું હતું પણ જનતા કર્ફ્યુ અને પછી લૉકડાઉનને કારણે તેમણે ત્યાં જ રહી દસ દિવસ વધુ રોકાવાનું નક્કી કર્યું હતું. તેઓ કહે છે, \"મારી બે પુત્રી છે અને એક નાનો પુત્ર. આઠ વર્ષનો પુત્ર દરરોજ ફોન પર કહે છે કે અબ્બા ક્યારે આવશો? \" ઇમરાનભાઈનું કહેવું છે. \"આ વખતે તો ઈદ પણ બાળકો વગર સરહદ પાર ઉજવવી પડી. હવે તો બસ રાહ જોઈએ છીએ કે ક્યારે અમને અમારા ઘરે જવાનો મોકો મળે. માતાપિતા, ભાઈ બહેન બાળકો બધાં જ ત્યાં અમારી રાહ જોઈ રહ્યાં છે. \" ઇમરાનભાઈ જણાવે છે કે પાકિસ્તાનના કરાચીમાં ઈદની નમાજ પઢવા તેઓ મસ્જિદમાં ગયા હતા. મસ્જિદમાં સોશિયલ ડિસ્ટન્સિંગનું ધ્યાન રાખીને નમાજ પઢવામાં આવી હતી. પાકિસ્તાનમાં લૉકડાઉનમાં છૂટછાટ પાકિસ્તાનમાં ઈદ અને રમઝાનમાં બજાર ખોલી દેવામાં આવ્યાં હતાં. ત્યારબાદ ત્યાં ભીડ પણ જોવા મળી હતી અને ત્યાં કોરોના સંક્રમણ વધવાના પણ અહેવાલ આવ્યા હતા. 20 મેથી પાકિસ્તાનમાં આંશિક રીતે રેલસેવા પણ શરૂ કરવામાં આવી હતી. જૉન્સ હૉપકિન્સ યુનિવર્સિટી પ્રમાણે, પાકિસ્તાનમાં અત્યાર સુધી કોરોના સંક્રમણના 66 હજારથી વધારે કેસ આવ્યા છે અને 1395 મૃત્યુ થયાં છે. ઇશાક બોકડાનું કહેવું છે કે અન્ય દેશોમાંથી ભારતના નાગરિકોને વતન પાછા લાવવામાં આવ્યા છે તો પાકિસ્તાનમાંથી તેમને લાવવામાં આવે. ઇશાકભાઈએ ગોધરાથી બીબીસીના સહયોગી દક્ષેશ શાહને એક ઈમેલ પણ મોકલ્યો છે, જેમાં ભારતીય વિદેશમંત્રાલયને 26 ભારતીય નાગરિકો અંગેની માહિતી અને ચાર જૂનની \\'ગોલ્ડન ટેમ્પલ ટ્રેન\\'ની ટિકિટની કૉપી મોકલીને તેમને પાકિસ્તાનથી ભારતમાં આવવાની પરવાનગી માટે મદદ કરવા જણાવાયું છે. ઇશાકભાઈનું કહેવું છે કે જો તેમને ચોક્કસ માહિતી આપવામાં આવે તો તેઓ કરાચીથી લાહોર જવા માટે સ્થાનિક પ્રશાસન પાસેથી ડિપાર્ચર પાસ માટે અરજી કરી શકે અને અટારી-વાઘા સુધીની યાત્રાની વ્યવસ્થા કરી શકે. ઉલ્લેખનીય છે કે ભારતમાં તારીખ 31 મેથી લૉકડાઉન પૂર્ણ થઈ રહ્યું છે અને એ બાદ અનલૉક-1 અંતર્ગત લૉકડાઉનની મુદ્દત વધારવામાં આવી છે. જોકે, આ દરમિયાન \\'વંદે ભારત મિશન\\'માં 45 હજારથી વધુ ભારતીયોને વિદેશમાંથી વતન લાવવામાં આવ્યા છે. વંદે ભારત મિશનમાં 45 હજારથી વધારે ભારતીય વતન આવ્યા ભારત સરકાર દ્વારા આ અભિયાન હેઠળ કોરોના મહામારી વચ્ચે વિદેશમાં ફસાયેલા ભારતીયોને પરત લાવવા માટેની કામગીરી ચલાવવામાં આવી રહી છે. નોંધનીય છે કે કોરોના સંક્રમણ વધતાં આંતરરાષ્ટ્રીય યાત્રા બંધ કરવામાં આવી હતી, જેથી અનેક દેશોમાં ભારતીય નાગરિકો ફસાયેલા છે. સમાચાર એજન્સી પીટીઆઈ મુજબ વિદેશ મંત્રાલયના પ્રવક્તા અનુરાગ શ્રીવાસ્તવે જણાવ્યું કે \"વંદે ભારત મિશન હેઠળ અત્યાર સુધી વિદેશમાં ફસાયેલા 45 હજાર જેટલા ભારતીય નાગરિકોને પરત લાવવામાં આવ્યા હતા.\" તેમણે એમ પણ જણાવ્યું કે 13 જૂન સુધીમાં બીજા એક લાખ જેટલા ભારતીયોને પરત લાવવામાં આવશે. વિદેશમાં કોરોના મહામારીને કારણે ફસાયેલા ભારતીય નાગરિકોને સ્વદેશ લાવવા માટે ભારત સરકારે વંદે ભારત મિશન તારીખ સાત મેના રોજ લૉન્ચ કર્યું હતું. આફ્રિકા, લૅટિન અમેરિકાના દેશો અને યુરોપના કેટલાક ભાગોમાંથી ભારતીય નાગરિકોને વતન લાવવાની કામગીરી હાથ ધરવામાં આવી છે. અનુરાગ શ્રીવાસ્તવે ગુરુવારે જણાવ્યું હતું કે અત્યાર સુધી 45,216 ભારતીયોને વતન લાવવામાં આવ્યા હતા, જેમાંથી 8,069 પ્રવાસી શ્રમિકો, 7656 વિદ્યાર્થીઓ અને 5,107 વ્યવસાયિકોને પાછા લાવવામાં આવ્યા છે. પાડોશી દેશો નેપાળ અને બાંગ્લાદેશમાંથી જમીનના રસ્તે સરહદ પાર કરીને પાંચ હજાર જેટલા ભારતીયો વતન પાછા ફર્યા છે. મીડિયામાં આવેલા અહેવાલ મુજબ પાકિસ્તાનમાં ફસાયેલા 300 ભારતીય નાગરિકોને શનિવારે ભારત પાછા લાવવામાં આવશે. મીડિયા અહેવાલો મુજબ આ ભારતીયો અટારી-વાઘા બૉર્ડર પરથી સરહદ પાર કરીને વતન પાછા ફરશે. જોકે બીબીસી આ દાવાની પુષ્ટિ નથી કરી શક્યું. ભારતથી વતન પાછા ફર્યા પાકિસ્તાની નાગરિકો સમાચાર એજન્સી પીટીઆઈ મુજબ તારીખ 27 મેના દિવસે બાળકો સહિત 179 પાકિસ્તાની નાગરિકો અટારી બૉર્ડરથી પાકિસ્તાન પાછા ગયા હતા. તેઓ લૉકડાઉનને કારણે ભારતમાં ફસાયા હતા. 179 પાકિસ્તાની નાગરિકોમાંથી કેટલાક લોકો મેડિકલ વિઝા પર ભારત આવ્યા હતા અને હૃદયરોગો, કિડની અને લીવરની બીમારીની સારવાર કરાવી રહ્યા હતા. કેટલાક લોકો ધાર્મિક યાત્રા પર હતા અને કેટલાક લોકો સંબંધીઓને મળવા આવ્યા હતા. પીટીઆઈ મુજબ તેમાંથી 120 હિંદુ, બે શીખ અને બાકી મુસ્લિમ લોકો હતા. આ લોકો ગુજરાત, મધ્ય પ્રદેશ, મહારાષ્ટ્ર, છત્તીસગઢ, દિલ્હી, હરિયાણા, ઉત્તર પ્રદેશ અને પંજાબ જેવાં રાજ્યોમાં ગયા હતા. આ પહેલાં લૉકડાઉન દરમિયાન પાંચમી મેના દિવસે 193 પાકિસ્તાની નાગરિકોને પાકિસ્તાન જવા દેવામાં આવ્યા હતા. તમે અમને ફેસબુક, ઇન્સ્ટાગ્રામ, યૂટ્યૂબ અને ટ્વિટર પર ફોલો કરી શકો છો']\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'][:1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "v24DAh0zwN8U"
   },
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "\n",
    "પ્રાચીનકાળમાં કાશ્મીર પર હિંદુ અને બૌદ્ધ સંસ્કૃતિઓનો વિશેષ પ્રભાવ પડેલ છે. એવુ માનવામા આવેછે કે અહીં ભગવાન શિવ ની પત્ની દેવી સતી નિવાસ કરતી હતી, અને તે સમયે આ ખીણ પાણીથી ઢંકાયેલી હતી, અહીં એક રાક્ષસ નાગ પણ રહેતો હતો, જેને વૈદીક ઋષિ કશ્યપ અને દેવી સતીએ મળીને હરાવ્યો હતો તથા મોટાભાગનુ પાણી ઝેલમ નદી ના રસ્તે વહાવી દીધુ હતુ. આ પ્રમાણે આ જગ્યા નુ નામ સતીસર થી કાશ્મીર પડ્યુ. આના થી તર્કસંગત પ્રસંગ એ પણ છે કે આનુ વાસ્તવિક નામ કશ્યપમર (અથવા કચબાનુ સરોવર) હતુ. આથી કાશ્મીર નામ પડ્યુ.\n",
    "\n",
    "કાશ્મીર નો સરસ પ્રાચીન ઇતિહાસ કલ્હણ (અને ત્યાર બાદ ના અન્ય લેખકોં) ના ગ્રંથ રાજતરંગિણી થી મળેછે. પ્રાચીન કાળમા અહીં હિંદુ આર્ય રાજાઓં નુ રાજ હતુ.\n",
    "\n",
    "મૌર્ય સમ્રાટ અશોક અને કુષાણ સમ્રાટ કનિષ્ક ના સમયમાં કાશ્મીર બૌદ્ધ ધર્મ અને સંસ્કૃતિ નુ મુખ્ય કેન્દ્ર બની ગયુ. પૂર્વ-મધ્યયુગ મા અહીંના ચક્રવર્તી સમ્રાટ લલિતાદિત્ય એ એક વિશાલ સામ્રાજ્ય ક઼ાયમ કરી લીધુ હતુ. કાશ્મીર સંસ્કૃત વિદ્યા નુ વિખ્યાત કેન્દ્ર હતુ.[૧]\n",
    "\n",
    "કાશ્મીર શૈવદર્શન પણ અહીં જન્મ્યા અને મોટા થયા. અહીંના મહાન મનીષીયોં માં પતઞ્જલિ, દૃઢબલ, વસુગુપ્ત, આનન્દવર્ધન, અભિનવગુપ્ત, કલ્હણ, ક્ષેમરાજ વગેરે છે. એવી ધારણા છે કે વિષ્ણુધર્મોત્તર પુરાણ તથા યોગ વાસિષ્ઠ અહીં લખાયેલ.\n",
    "મધ્યયુગ માં મુસ્લિમ હુમલાખોર કાશ્મીર પર હાવી થઇ ગયા. કેટલાક મુસલમાન શાહ અને રાજ્યપાલ (જેવાકે શાહ જ઼ૈન-ઉલ-અબિદીન) હિન્દુઓં સાથે સારો વ્યવહાર કરતા હતા. પરંતુ કેટલાક (જેવાકે સુલ્તાન સિકંદર બુતશિકન) એ અહીંના મૂળ કાશ્મીરી હિન્દુઓં ને મુસલમાન બનવા, અથવા રાજ્ય છોડ઼વા કે મરવા માટે મજબૂર કર દિધા. થોડીક સદીઓમા કાશ્મીર ઘાટીમા મુસ્લિમ બહુમત થઇ ગયુ. મુસલમાન શાહોમા વાર્ંવાર અફ઼ગ઼ાન, કાશ્મીરી મુસલમાન, મુગ઼લ આદિ વંશોં પાસે ગયુ. મુગ઼લ સલ્તનત ના પતન પછી શીખ મહારાજા રણજીત સિંહ ના રાજ્ય મા ભળી થઇ ગયા. થોડા સમય બાદ જમ્મૂ ના હિંદુ ડોગરા રાજા ગુલાબ સિંહ ડોગરા એ બ્રિટિશ લોકો સાથે સંધિ કરીને જમ્મૂ ની સાથે સાથે કાશ્મીર પર પણ અધિકાર કરી લીધો ( જેને એવુ પણ કહેવાયકે કાશ્મીર ને ખ઼રીદી લીધુ )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "QvJEpqqqsd_4",
    "outputId": "aeff6e1d-67e0-4468-acf5-d9fc2f4562de"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nપ્રાચીનકાળમાં કાશ્મીર પર હિંદુ અને બૌદ્ધ સંસ્કૃતિઓનો વિશેષ પ્રભાવ પડેલ છે. એવુ માનવામા આવેછે કે અહીં ભગવાન શિવ ની પત્ની દેવી સતી નિવાસ કરતી હતી, અને તે સમયે આ ખીણ પાણીથી ઢંકાયેલી હતી, અહીં એક રાક્ષસ નાગ પણ રહેતો હતો, જેને વૈદીક ઋષિ કશ્યપ અને દેવી સતીએ મળીને હરાવ્યો હતો તથા મોટાભાગનુ પાણી ઝેલમ નદી ના રસ્તે વહાવી દીધુ હતુ. આ પ્રમાણે આ જગ્યા નુ નામ સતીસર થી કાશ્મીર પડ્યુ. આના થી તર્કસંગત પ્રસંગ એ પણ છે કે આનુ વાસ્તવિક નામ કશ્યપમર (અથવા કચબાનુ સરોવર) હતુ. આથી કાશ્મીર નામ પડ્યુ.\\n\\nકાશ્મીર નો સરસ પ્રાચીન ઇતિહાસ કલ્હણ (અને ત્યાર બાદ ના અન્ય લેખકોં) ના ગ્રંથ રાજતરંગિણી થી મળેછે. પ્રાચીન કાળમા અહીં હિંદુ આર્ય રાજાઓં નુ રાજ હતુ.\\n\\nમૌર્ય સમ્રાટ અશોક અને કુષાણ સમ્રાટ કનિષ્ક ના સમયમાં કાશ્મીર બૌદ્ધ ધર્મ અને સંસ્કૃતિ નુ મુખ્ય કેન્દ્ર બની ગયુ. પૂર્વ-મધ્યયુગ મા અહીંના ચક્રવર્તી સમ્રાટ લલિતાદિત્ય એ એક વિશાલ સામ્રાજ્ય ક઼ાયમ કરી લીધુ હતુ. કાશ્મીર સંસ્કૃત વિદ્યા નુ વિખ્યાત કેન્દ્ર હતુ.[૧]\\n\\nકાશ્મીર શૈવદર્શન પણ અહીં જન્મ્યા અને મોટા થયા. અહીંના મહાન મનીષીયોં માં પતઞ્જલિ, દૃઢબલ, વસુગુપ્ત, આનન્દવર્ધન, અભિનવગુપ્ત, કલ્હણ, ક્ષેમરાજ વગેરે છે. એવી ધારણા છે કે વિષ્ણુધર્મોત્તર પુરાણ તથા યોગ વાસિષ્ઠ અહીં લખાયેલ.\\nમધ્યયુગ માં મુસ્લિમ હુમલાખોર કાશ્મીર પર હાવી થઇ ગયા. કેટલાક મુસલમાન શાહ અને રાજ્યપાલ (જેવાકે શાહ જ઼ૈન-ઉલ-અબિદીન) હિન્દુઓં સાથે સારો વ્યવહાર કરતા હતા. પરંતુ કેટલાક (જેવાકે સુલ્તાન સિકંદર બુતશિકન) એ અહીંના મૂળ કાશ્મીરી હિન્દુઓં ને મુસલમાન બનવા, અથવા રાજ્ય છોડ઼વા કે મરવા માટે મજબૂર કર દિધા. થોડીક સદીઓમા કાશ્મીર ઘાટીમા મુસ્લિમ બહુમત થઇ ગયુ. મુસલમાન શાહોમા વાર્ંવાર અફ઼ગ઼ાન, કાશ્મીરી મુસલમાન, મુગ઼લ આદિ વંશોં પાસે ગયુ. મુગ઼લ સલ્તનત ના પતન પછી શીખ મહારાજા રણજીત સિંહ ના રાજ્ય મા ભળી થઇ ગયા. થોડા સમય બાદ જમ્મૂ ના હિંદુ ડોગરા રાજા ગુલાબ સિંહ ડોગરા એ બ્રિટિશ લોકો સાથે સંધિ કરીને જમ્મૂ ની સાથે સાથે કાશ્મીર પર પણ અધિકાર કરી લીધો ( જેને એવુ પણ કહેવાયકે કાશ્મીર ને ખ઼રીદી લીધુ )\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aguokS2z2mC-"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(test_text, return_tensors=\"pt\",truncation=True, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwKK1a9X3U6R",
    "outputId": "059caef9-bb23-40e0-825f-239cea6c612b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  572,  6576,  2205,   139,   642,   110,  1762,  8426,   120,  1061,\n",
       "          1024,   518,   206,   108,   106,  6697,  6538,   280,   128,  2662,\n",
       "           122,   274,   597,  1004,   377,  1600,  1490,  7017,  3325,   763,\n",
       "           140,   107,   110,   131,   415,   114,  2359,   500,   127, 12173,\n",
       "           136,   140,   107,   274,   117,  7738,  2947,   125,  6955,   151,\n",
       "           107,   432,  4727,   953,   191,  2533, 15824,   110,  1490,  7017,\n",
       "           149,  2532,  6811,   151,   240,  5929,   614,   500,  4292,  3164,\n",
       "           387,   213, 11253, 12136,   361, 17396,  1476,   106,   114,   443,\n",
       "           114,  1285,  3574,   249,  7017,   678,   332,  2205,  1993,   640,\n",
       "           106,   114,   113,   332, 19425,  5291,   150,   125,   108,   122,\n",
       "           114,   614,  2333,   249, 15824,  2204,   109,   103,  6646,   601,\n",
       "           424,   606,   614,  2731,   963,  1476,   106,  2358,  2205,   249,\n",
       "          1993,   640,   106,  2205,   371,  5288,   572,   934, 11987,   436,\n",
       "           331,   109,   103,  1511,   616,   250,   213,   166,  2874,   162,\n",
       "           963,   213,  1335,   573,  1083,   431, 10485,   332,   316,  2662,\n",
       "           106,   572,  1645,   280,   274,   642,  5930,  2867,   162,  3574,\n",
       "           573,  1476,   106,  8600,  3099,  3511,   110, 18554,  3099, 15237,\n",
       "           213,   571,  2205,  1762,   506,   110,   986,  3574,   143,   551,\n",
       "           372, 13057,   106,   297,   137,  8626,  3468,   537,   758, 16001,\n",
       "          3099,  8673, 10262,  2807,   150,   117, 18751,  1247,   601,  3575,\n",
       "          1036,   214,   164, 12224,  1476,   106,  2205,  1406,  3130,  3574,\n",
       "          3127,   551,  1476,   106,   103,  1730,  5449,  2205, 13321,  4253,\n",
       "           125,   274,  8783,   110,   265,   355,   106,   758,  1206,   525,\n",
       "           111,  1545,  3630,   162,   237,   109,  3766, 13016,  8458,   880,\n",
       "           107, 11957,  2690,   107, 11730,  7757,   107,   114,   193,  3173,\n",
       "          9259,   107, 14464,   300,  7757,   107, 11987,   436,   331,   107,\n",
       "          6180,   133,   214,  1155,   310,   108,   106,   492,  3240,   108,\n",
       "           122,  1995,  3184, 10242,  2265,   240,  2004,  5370, 13081,   274,\n",
       "          7715,   106,  9779,   237,  1559, 11869,  2205,   139,  2357,   361,\n",
       "           322,   404,   106,   267,  7754,  1421,   110,  9139,   109,   103,\n",
       "           671,   285,   504,  1421,   109,  7714, 11920,   137,  6183,   137,\n",
       "           497,  2374,   953,   193,   963, 10328,   162,   158,  3715,  2773,\n",
       "           259,   147,   106,   248,   267,   109,   103,   671,   285,   504,\n",
       "          8984, 16103,  2370,   257,  1773,  3587,   963,   150,   758,   457,\n",
       "          2205,   136, 10328,   162,   305,  7754,  2697,   107,   212,   323,\n",
       "          1042,  3575,   285,   122,  2746,   285,   126, 12700,   689,  3001,\n",
       "          1445,   106, 13360,  4275,   280,  2205, 11804,   280,  1559, 15787,\n",
       "           322, 13057,   106,  7754,  1421,   132,   280,   773,   396,   162,\n",
       "          1370,   243, 10067,   383,  3575,  2877,   107,  2205,   136,  7754,\n",
       "           107,   898,   383,  3575,   206,  1305,  2035,  2994,   448, 13057,\n",
       "           106,   898,   383,  3575,   206,  7129,   213,  4407,   235,  1479,\n",
       "          2603, 16448,  1887,   213,   323,   537,  4956,   322,   404,   106,\n",
       "           909,   330,   250,   116,  5271,   850,   213,   642, 17564,   522,\n",
       "          2889,  1887, 17564,   150,   693,   198,   158,  2318,   247,   116,\n",
       "          5271,   850,   377,   158,   158,  2205,   139,   125,  1992,   164,\n",
       "           936,   109,   103,   432,  6697,   125,   499,   504,  2205,   305,\n",
       "          1115,  3575,   320,   953, 12224,  1080,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(inputs['input_ids'][0]))\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ViEO3Q4WVRK7",
    "outputId": "82b5cb29-687b-4460-bd4b-40bf9a03282e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  572,  6576,  2205,   139,   642,   110,  1762,  8426,   120,  1061,\n",
      "          1024,   518,   206,   108,   106,  6697,  6538,   280,   128,  2662,\n",
      "           122,   274,   597,  1004,   377,  1600,  1490,  7017,  3325,   763,\n",
      "           140,   107,   110,   131,   415,   114,  2359,   500,   127, 12173,\n",
      "           136,   140,   107,   274,   117,  7738,  2947,   125,  6955,   151,\n",
      "           107,   432,  4727,   953,   191,  2533, 15824,   110,  1490,  7017,\n",
      "           149,  2532,  6811,   151,   240,  5929,   614,   500,  4292,  3164,\n",
      "           387,   213, 11253, 12136,   361, 17396,  1476,   106,   114,   443,\n",
      "           114,  1285,  3574,   249,  7017,   678,   332,  2205,  1993,   640,\n",
      "           106,   114,   113,   332, 19425,  5291,   150,   125,   108,   122,\n",
      "           114,   614,  2333,   249, 15824,  2204,   109,   103,  6646,   601,\n",
      "           424,   606,   614,  2731,   963,  1476,   106,  2358,  2205,   249,\n",
      "          1993,   640,   106,  2205,   371,  5288,   572,   934, 11987,   436,\n",
      "           331,   109,   103,  1511,   616,   250,   213,   166,  2874,   162,\n",
      "           963,   213,  1335,   573,  1083,   431, 10485,   332,   316,  2662,\n",
      "           106,   572,  1645,   280,   274,   642,  5930,  2867,   162,  3574,\n",
      "           573,  1476,   106,  8600,  3099,  3511,   110, 18554,  3099, 15237,\n",
      "           213,   571,  2205,  1762,   506,   110,   986,  3574,   143,   551,\n",
      "           372, 13057,   106,   297,   137,  8626,  3468,   537,   758, 16001,\n",
      "          3099,  8673, 10262,  2807,   150,   117, 18751,  1247,   601,  3575,\n",
      "          1036,   214,   164, 12224,  1476,   106,  2205,  1406,  3130,  3574,\n",
      "          3127,   551,  1476,   106,   103,  1730,  5449,  2205, 13321,  4253,\n",
      "           125,   274,  8783,   110,   265,   355,   106,   758,  1206,   525,\n",
      "           111,  1545,  3630,   162,   237,   109,  3766, 13016,  8458,   880,\n",
      "           107, 11957,  2690,   107, 11730,  7757,   107,   114,   193,  3173,\n",
      "          9259,   107, 14464,   300,  7757,   107, 11987,   436,   331,   107,\n",
      "          6180,   133,   214,  1155,   310,   108,   106,   492,  3240,   108,\n",
      "           122,  1995,  3184, 10242,  2265,   240,  2004,  5370, 13081,   274,\n",
      "          7715,   106,  9779,   237,  1559, 11869,  2205,   139,  2357,   361,\n",
      "           322,   404,   106,   267,  7754,  1421,   110,  9139,   109,   103,\n",
      "           671,   285,   504,  1421,   109,  7714, 11920,   137,  6183,   137,\n",
      "           497,  2374,   953,   193,   963, 10328,   162,   158,  3715,  2773,\n",
      "           259,   147,   106,   248,   267,   109,   103,   671,   285,   504,\n",
      "          8984, 16103,  2370,   257,  1773,  3587,   963,   150,   758,   457,\n",
      "          2205,   136, 10328,   162,   305,  7754,  2697,   107,   212,   323,\n",
      "          1042,  3575,   285,   122,  2746,   285,   126, 12700,   689,  3001,\n",
      "          1445,   106, 13360,  4275,   280,  2205, 11804,   280,  1559, 15787,\n",
      "           322, 13057,   106,  7754,  1421,   132,   280,   773,   396,   162,\n",
      "          1370,   243, 10067,   383,  3575,  2877,   107,  2205,   136,  7754,\n",
      "           107,   898,   383,  3575,   206,  1305,  2035,  2994,   448, 13057,\n",
      "           106,   898,   383,  3575,   206,  7129,   213,  4407,   235,  1479,\n",
      "          2603, 16448,  1887,   213,   323,   537,  4956,   322,   404,   106,\n",
      "           909,   330,   250,   116,  5271,   850,   213,   642, 17564,   522,\n",
      "          2889,  1887, 17564,   150,   693,   198,   158,  2318,   247,   116,\n",
      "          5271,   850,   377,   158,   158,  2205,   139,   125,  1992,   164,\n",
      "           936,   109,   103,   432,  6697,   125,   499,   504,  2205,   305,\n",
      "          1115,  3575,   320,   953, 12224,  1080,     1]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(test_text, return_tensors=\"pt\",truncation=True, padding=\"longest\").input_ids\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ttPRmWfIDl-7"
   },
   "outputs": [],
   "source": [
    "output_ids = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt81EfvcozA7",
    "outputId": "b7f402b3-ce56-440c-87b1-fdeecf888e2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 130])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0oWsCCFTDMPU"
   },
   "outputs": [],
   "source": [
    "final_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zf7XzYm9DWKj",
    "outputId": "feafc6a1-f2b3-4ccf-a97d-a0476910e24e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['જેને વૈદીક ઋષિ કશ્યપ અને દેવી સતીએ મળીને હરાવ્યો હતો તથા મોટાભાગનુ પાણી ઝેલમ નદી ના રસ્તે વહાવી દીધુ હતુ,. એવી ધારણા છે કે વિષ્ણુધર્મોત્તર પુરાણ તથા યોગ વાસિષ્ઠ અહીં લખાયેલ,. મધ્યયુગ માં મુસ્લિમ હુમલાખોર કાશ્મીર પર હાવી થઇ ગયા, કેટલાક મુસલમાન શાહ અને રાજ્યપાલ  )જેવાકે શાહ જ઼ૈન-ઉલ-અબિદીન) હિન્દુઓં સાથે સારો વ્યવહાર કરતા હતા. થોડા સમય બાદ જમ્મૂ ના હિંદુ ડોગરા રાજા ગુલાબ સિંહ ડોગરા એ બ્રિટિશ લોકો સાથે સંધિ કરીને જમ્મૂ ની સાથે સાથે કાશ્મીર પર પણ અધિકાર કરી લીધો  જેને એવુ પણ કહેવાયકે કાશ્મીર ને ખ઼રીદી']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "\n",
    "- It gave us gujarati summary as output but, that summary was not accurate.\n",
    "- It picked the senteces inbetween the text as it was pre-trained on GSG task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BV-JnJlz50lj",
    "outputId": "4845d9ed-659d-48c4-caef-e80da9b571a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# output_sequences = model.generate(decoder_input_ids =inputs['input_ids'],bos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "q0E39pCD3tkW"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# output_sequences = model.generate(\n",
    "#     input_ids=inputs['input_ids'],\n",
    "#     attention_mask=inputs['attention_mask'],\n",
    "#     num_beams=3,\n",
    "#     no_repeat_ngram_size=2,\n",
    "#     min_length=30,\n",
    "#     max_length=150,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E4ffuKA6veUl"
   },
   "outputs": [],
   "source": [
    "# option 1\n",
    "# ###########################################################################################################\n",
    "# from fastai.text import *\n",
    "\n",
    "# class GujaratiTokenizer(BaseTokenizer):\n",
    "#     def __init__(self, lang:str):\n",
    "#         self.lang = lang\n",
    "#         self.sp = spm.SentencePieceProcessor()\n",
    "#         self.sp.Load(str('/content/drive/MyDrive/Tokenizer/gujarati_lm.model'))\n",
    "        \n",
    "#     def tokenizer(self, t:str) -> List[str]:\n",
    "#         return self.sp.EncodeAsPieces(t)\n",
    "\n",
    "# t = GujaratiTokenizer(lang='gu')\n",
    "# output = t.tokenizer(test_text)\n",
    "# ###########################################################################################################\n",
    "\n",
    "# option 2\n",
    "\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(str('/content/drive/MyDrive/Tokenizer/gujarati_lm.model'))\n",
    "# itos = [sp.IdToPiece(int(i)) for i in range(20000)] #Vocabulary size (20,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XOH1SJEMwX_c"
   },
   "outputs": [],
   "source": [
    "# Token to id and id to token again :)\n",
    "\n",
    "# print(sp.EncodeAsPieces(test_text))\n",
    "# encoding = sp.EncodeAsIds(test_text)\n",
    "# print(encoding)\n",
    "# output = sp.DecodeIds(encoding)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zlQDnPJZyD3E"
   },
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYZM9WXWeP3w"
   },
   "source": [
    "---\n",
    "# **Fine-tuning with XSum gujarati text :**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTW5L6Zy0CO8",
    "outputId": "94275e3a-611d-4764-d013-dc6c66544dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.19.5)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install rouge_score\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-n6gButxMEC",
    "outputId": "f4fb75fc-bfa6-47aa-ad98-05e5d0dea149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments, Seq2SeqTrainingArguments,Seq2SeqTrainer\n",
    "import torch\n",
    "from datasets import load_dataset, list_datasets,load_metric\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "GHk7_49DeXJD"
   },
   "outputs": [],
   "source": [
    "# converting our encodings in to Dataset objects\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "uOuReXZ4wKvz"
   },
   "outputs": [],
   "source": [
    "# Input : model-name, input-text(X), label(y) \n",
    "# 1.) Take our text data\n",
    "# 2.) Apply tokenizer on it according to our model\n",
    "# 3.) encodings : Convert thoes tokens to numbers\n",
    "# 4.) Prepare DataSet from thoes encodings\n",
    "# return that DataSet object\n",
    "\n",
    "def prepare_data(model_name, \n",
    "                 train_texts, train_labels, \n",
    "                 val_texts=None, val_labels=None, \n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "\n",
    "  # create tokenizer for our model\n",
    "  tokenizer = PegasusTokenizer(vocab_file='/content/drive/MyDrive/Tokenizer/gujarati_lm.model',name_or_path='PEGASUS', model_max_length=1024,bos_token='<s>')\n",
    "  \n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "# create encodings from out text data\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "# create combinded dataset object from text and label encodings\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "TvcXQav2wQd2"
   },
   "outputs": [],
   "source": [
    "def prepare_fine_tuning(model_name, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "  \n",
    "  \"\"\"\n",
    "  Prepare configurations and base model for fine-tuning\n",
    "  \"\"\"\n",
    "  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "  tokenizer = PegasusTokenizer(vocab_file='/content/drive/MyDrive/Tokenizer/gujarati_lm.model',name_or_path='PEGASUS', model_max_length=1024,bos_token='<s>')\n",
    "  model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "  # batch = tokenizer.prepare_seq2seq_batch(in_text, truncation=True, padding='longest').to(torch_device) \n",
    "\n",
    "  if freeze_encoder:\n",
    "    for param in model.model.encoder.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  if val_dataset is not None:\n",
    "    pass\n",
    "\n",
    "  else:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=output_dir,           # output directory\n",
    "      per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "      predict_with_generate=True,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "      model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      tokenizer=tokenizer,\n",
    "      compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "RB-1MkkyzpkY"
   },
   "outputs": [],
   "source": [
    "tokenizer = PegasusTokenizer(vocab_file='/content/drive/MyDrive/Tokenizer/gujarati_lm.model',name_or_path='PEGASUS', model_max_length=1024,bos_token='<s>')\n",
    "metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yf6rpRukzk6A"
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "    \n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qi3v5d-MxP2c",
    "outputId": "8d812a5e-59ff-43a7-ca97-c802d899e639"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xlsum (/root/.cache/huggingface/datasets/GEM___xlsum/gujarati/2.0.0/c5f94b79254b76efed292f24957fe663c4b35c83e91284fbefc51adf4aea8dc0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"GEM/xlsum\",'gujarati',split='test[:100]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "tlgPEe9Jx_cd"
   },
   "outputs": [],
   "source": [
    "lable, text = ds['target'],ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL3-ygxQyAlz",
    "outputId": "accf7e7d-551d-4e60-e6d5-f1407dc3bf56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "SQ74pK-sytHB"
   },
   "outputs": [],
   "source": [
    "model_name = 'google/pegasus-large'\n",
    "\n",
    "test_dataset,_,_ = prepare_data(model_name, text, lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9HXZrS-zCcZ",
    "outputId": "736af290-02cd-48f3-c5ca-494f112d793b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 3.51 s, total: 15.6 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = prepare_fine_tuning(model_name,test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "goV01qR7zV0J",
    "outputId": "c7ec5fb2-9132-4aa4-9d20-79a869d7e2a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 100\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 2:18:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_results = trainer.predict(test_dataset,metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FFlcLAz5EY-",
    "outputId": "9c4cb287-60dc-499d-e96f-425571eeca1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict_gen_len': 86.32,\n",
       " 'predict_loss': 9.815193176269531,\n",
       " 'predict_rouge1': 1.6667,\n",
       " 'predict_rouge2': 0.0,\n",
       " 'predict_rougeL': 1.6667,\n",
       " 'predict_rougeLsum': 1.8333,\n",
       " 'predict_runtime': 8420.2138,\n",
       " 'predict_samples_per_second': 0.012,\n",
       " 'predict_steps_per_second': 0.012}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "sTvP8aQx5RLg"
   },
   "outputs": [],
   "source": [
    "final_output = tokenizer.batch_decode(predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCE9oIL_5SKR",
    "outputId": "3cf82952-c31a-46de-d7a9-f23df9845fe8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['મુખ્યમંત્રીએ કહ્યું હતું કે નર્મદાના પાણી પર નભતા રાજ્યના 10 હજાર થી વધુ ગામડા અને 167 જેટલા નગરોને પીવાના પાણીની તકલીફ ન પડે તેટલા માટે ઉનાળુ પાક ખેડૂતો ન કરે તેવી અપેક્ષા રખાય છે. તેમણે કહ્યું હતું કે રાજય સરકારે ચોમાસું અને શિયાળુ પાક માટે પાણી આપ્યું હતું અને શિયાળુ પાક પર કોઈ અસર ન પડે તેટલા માટે ઉનાળામાં પાણી ન આપવાનો નિર્ણય કર્યો છે.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5qZuPIEdoVU",
    "outputId": "faadce35-8022-4d6c-a738-f66045eb2d8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['દિવ્યભાસ્કરમાં પ્રકાશિત અહેવાલ મુજબ મુખ્યમંત્રી વિજય રૂપાણીએ ખેડૂતોને ઉનાળુ પાક ન કરવાની ચેતવણી આપી હતી.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ciy825-dxwV"
   },
   "source": [
    "# Observation :\n",
    "\n",
    "- ROUGE score was not good.\n",
    "- It generated summary, but it was not accurate. It was not coherent.\n",
    "- but compared with direct inference, it was better"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhvX5qnqP6qjaZ3C+T9RgX",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1nUk9I8ZscsnEpO_bV02WoiEPbY7BoK9L",
   "name": "Pegasus-on-GujData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02c0113144ca4a3c801ee16450bd23fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16f9e10417b849d88df2d9a8f951f5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cfc34fabd4d4950a74083cfbd3ae5e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "669539e1d571432cbcd46302c56fd3a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e31b9935da74636985f5b7d550291d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b08b8a13f1648c99ff74245413a81fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e31b9935da74636985f5b7d550291d8",
      "placeholder": "​",
      "style": "IPY_MODEL_bf84befce4064e6d8bad584a58f2148e",
      "value": " 3/3 [00:00&lt;00:00,  6.87it/s]"
     }
    },
    "9fa56ee3d68c4c769e616d43f81357ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2b9ae59eeff477b967990aea69d5638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dab4563495f644c48c3a8641df375a86",
       "IPY_MODEL_d7c802ccaf214080aca4b6b8742ded3b",
       "IPY_MODEL_8b08b8a13f1648c99ff74245413a81fd"
      ],
      "layout": "IPY_MODEL_02c0113144ca4a3c801ee16450bd23fb"
     }
    },
    "bf84befce4064e6d8bad584a58f2148e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7c802ccaf214080aca4b6b8742ded3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fa56ee3d68c4c769e616d43f81357ff",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cfc34fabd4d4950a74083cfbd3ae5e8",
      "value": 3
     }
    },
    "dab4563495f644c48c3a8641df375a86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_669539e1d571432cbcd46302c56fd3a1",
      "placeholder": "​",
      "style": "IPY_MODEL_16f9e10417b849d88df2d9a8f951f5dd",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
